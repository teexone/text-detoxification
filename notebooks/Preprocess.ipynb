{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.11/site-packages (0.1.99)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /root/miniconda3/lib/python3.11/site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(os.getcwd(), '..', 'data')\n",
    "DATASET_FILE = os.path.join(DATA_FOLDER, 'raw', 'filtered.tsv')\n",
    "MODEL_FOLDER = os.path.join(os.getcwd(), '..', 'models')\n",
    "MODEL_PREFIX = os.path.join(MODEL_FOLDER, 'tokenizer')\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The notebook contains the code for preprocessing the data. The data is split and sentences are processed with tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentences number: 1155554'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATASET_FILE, sep='\\t')\n",
    "corpus = data['reference'].to_list() + data['translation'].to_list()\n",
    "\n",
    "\"Sentences number: {}\".format(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/shared/detoxification/notebooks/../data/interim/corpus.txt --model_prefix=/shared/detoxification/notebooks/../models/tokenizer --vocab_size=10000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /shared/detoxification/notebooks/../data/interim/corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: /shared/detoxification/notebooks/../models/tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /shared/detoxification/notebooks/../data/interim/corpus.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1155554), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1155554 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=61071738\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=70\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1155554 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=33916923\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 292990 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1155554\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 313105\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 313105 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=110629 obj=10.9029 num_tokens=705262 num_tokens/piece=6.37502\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=93768 obj=8.5698 num_tokens=713125 num_tokens/piece=7.60521\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=70320 obj=8.52105 num_tokens=742124 num_tokens/piece=10.5535\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=70295 obj=8.51363 num_tokens=743214 num_tokens/piece=10.5728\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=52720 obj=8.54405 num_tokens=785481 num_tokens/piece=14.8991\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=52720 obj=8.53709 num_tokens=785453 num_tokens/piece=14.8986\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39538 obj=8.58496 num_tokens=833026 num_tokens/piece=21.069\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=39538 obj=8.57617 num_tokens=832872 num_tokens/piece=21.0651\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=29653 obj=8.63782 num_tokens=882613 num_tokens/piece=29.7647\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=29653 obj=8.6267 num_tokens=882493 num_tokens/piece=29.7607\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=22239 obj=8.7052 num_tokens=933257 num_tokens/piece=41.9649\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=22239 obj=8.69147 num_tokens=933042 num_tokens/piece=41.9552\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16679 obj=8.78981 num_tokens=984406 num_tokens/piece=59.0207\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16679 obj=8.77297 num_tokens=984185 num_tokens/piece=59.0074\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12509 obj=8.89165 num_tokens=1035083 num_tokens/piece=82.7471\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12509 obj=8.87137 num_tokens=1034725 num_tokens/piece=82.7184\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11000 obj=8.92896 num_tokens=1058100 num_tokens/piece=96.1909\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11000 obj=8.9194 num_tokens=1057966 num_tokens/piece=96.1787\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /shared/detoxification/notebooks/../models/tokenizer.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /shared/detoxification/notebooks/../models/tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, 'interim', 'corpus.txt'), 'w+') as f:\n",
    "    f.write('\\n'.join(corpus))\n",
    "    \n",
    "SentencePieceTrainer.Train(\n",
    "    '--input={0} --model_prefix={1} --vocab_size={2}'.format(\n",
    "        os.path.join(DATA_FOLDER, 'interim', 'corpus.txt'),\n",
    "        MODEL_PREFIX,\n",
    "        VOCAB_SIZE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "sp = SentencePieceProcessor()\n",
    "sp.load(os.path.join(MODEL_PREFIX + '.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode(s):\n",
    "    return [sp.bos_id()] + sp.encode_as_ids(s) + [sp.eos_id()]\n",
    "\n",
    "data['reference'] = data['reference'].apply(encode)\n",
    "data['translation'] = data['translation'].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 154, 1229, 1304, 199, 24, 5760, 64, 49, 28...</td>\n",
       "      <td>[1, 44, 1229, 1304, 199, 5760, 6, 49, 28, 49, ...</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 324, 8, 5, 23, 418, 706, 3, 2]</td>\n",
       "      <td>[1, 8, 5, 23, 3730, 632, 3, 2]</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.065473</td>\n",
       "      <td>0.999039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 257, 4, 37, 107, 2714, 22, 160, 4, 32, 65,...</td>\n",
       "      <td>[1, 219, 4, 37, 55, 2714, 22, 160, 3, 2]</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.213313</td>\n",
       "      <td>0.985068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 1722, 18, 4959, 4, 8, 5, 85, 94, 11, 3585,...</td>\n",
       "      <td>[1, 812, 4, 8, 33, 11, 1264, 47, 3, 2]</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.994215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 9, 5, 85, 94, 1757, 11, 149, 49, 129, 3, 2]</td>\n",
       "      <td>[1, 9, 33, 1757, 11, 79, 49, 3, 2]</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.999348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          reference  \\\n",
       "0           0  [1, 154, 1229, 1304, 199, 24, 5760, 64, 49, 28...   \n",
       "1           1                 [1, 324, 8, 5, 23, 418, 706, 3, 2]   \n",
       "2           2  [1, 257, 4, 37, 107, 2714, 22, 160, 4, 32, 65,...   \n",
       "3           3  [1, 1722, 18, 4959, 4, 8, 5, 85, 94, 11, 3585,...   \n",
       "4           4    [1, 9, 5, 85, 94, 1757, 11, 149, 49, 129, 3, 2]   \n",
       "\n",
       "                                         translation  similarity  lenght_diff  \\\n",
       "0  [1, 44, 1229, 1304, 199, 5760, 6, 49, 28, 49, ...    0.785171     0.010309   \n",
       "1                     [1, 8, 5, 23, 3730, 632, 3, 2]    0.749687     0.071429   \n",
       "2           [1, 219, 4, 37, 55, 2714, 22, 160, 3, 2]    0.919051     0.268293   \n",
       "3             [1, 812, 4, 8, 33, 11, 1264, 47, 3, 2]    0.664333     0.309524   \n",
       "4                 [1, 9, 33, 1757, 11, 79, 49, 3, 2]    0.726639     0.181818   \n",
       "\n",
       "    ref_tox   trn_tox  \n",
       "0  0.014195  0.981983  \n",
       "1  0.065473  0.999039  \n",
       "2  0.213313  0.985068  \n",
       "3  0.053362  0.994215  \n",
       "4  0.009402  0.999348  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flips: 258635'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_data = data.copy()\n",
    "\n",
    "flips = 0\n",
    "for i, row in data.iterrows():\n",
    "    if row['ref_tox'] < row['trn_tox']:\n",
    "        flipped_data.at[i, 'reference'], flipped_data.at[i, 'translation'] = data.at[i, 'translation'], data.at[i, 'reference']\n",
    "        flipped_data.at[i, 'ref_tox'], flipped_data.at[i, 'trn_tox'] = data.at[i, 'trn_tox'], data.at[i, 'ref_tox']\n",
    "        flips += 1\n",
    "        \n",
    "\"Flips: {}\".format(flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(flipped_data['ref_tox'] >= flipped_data['trn_tox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_array = [np.array(f, dtype=np.uint16) for f in flipped_data['reference'].values]\n",
    "translation_array = [np.array(f, dtype=np.uint16) for f in flipped_data['translation'].values]\n",
    "ref_tox = flipped_data['ref_tox'].to_numpy().astype(np.float32)\n",
    "trn_tox = flipped_data['trn_tox'].to_numpy().astype(np.float32)\n",
    "\n",
    "assert len(reference_array) == len(translation_array) == ref_tox.shape[0] == trn_tox.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1409)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(reference_array), int(len(reference_array) * 0.8), replace=False)\n",
    "val_indices = np.setdiff1d(np.arange(len(reference_array)), train_indices)\n",
    "test_indices = np.random.choice(val_indices, int(len(val_indices) * 0.5), replace=False)\n",
    "val_indices = np.setdiff1d(val_indices, test_indices)\n",
    "\n",
    "assert len(train_indices) + len(val_indices) + len(test_indices) == len(reference_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(os.path.join(DATA_FOLDER, 'interim', 'train.pkl'), 'wb') as f:\n",
    "    refs = [reference_array[i] for i in train_indices]\n",
    "    trns = [translation_array[i] for i in train_indices]\n",
    "    pickle.dump(\n",
    "        {\n",
    "            'reference': refs,\n",
    "            'translation': trns,\n",
    "            'ref_tox': ref_tox[train_indices],\n",
    "            'trn_tox': trn_tox[train_indices]\n",
    "        },\n",
    "        f\n",
    "    )\n",
    "    \n",
    "with open(os.path.join(DATA_FOLDER, 'interim', 'val.pkl'), 'wb') as f:\n",
    "    refs = [reference_array[i] for i in val_indices]\n",
    "    trns = [translation_array[i] for i in val_indices]\n",
    "    pickle.dump(\n",
    "        {\n",
    "            'reference': refs,\n",
    "            'translation': trns,\n",
    "            'ref_tox': ref_tox[val_indices],\n",
    "            'trn_tox': trn_tox[val_indices]\n",
    "        },\n",
    "        f\n",
    "    )\n",
    "    \n",
    "    \n",
    "with open(os.path.join(DATA_FOLDER, 'interim', 'test.pkl'), 'wb') as f:\n",
    "    refs = [reference_array[i] for i in test_indices]\n",
    "    trns = [translation_array[i] for i in test_indices]\n",
    "    pickle.dump(\n",
    "        {\n",
    "            'reference': refs,\n",
    "            'translation': trns,\n",
    "            'ref_tox': ref_tox[test_indices],\n",
    "            'trn_tox': trn_tox[test_indices]\n",
    "        },\n",
    "        f\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of .tsv file: 103.27 MB\n",
      "Size of train.pkl file: 58.37 MB\n",
      "Size of val.pkl file: 7.28 MB\n",
      "Size of test.pkl file: 7.30 MB\n"
     ]
    }
   ],
   "source": [
    "print('Size of .tsv file: {:.2f} MB'.format(os.path.getsize(DATASET_FILE) / 1024 / 1024))\n",
    "print('Size of train.pkl file: {:.2f} MB'.format(os.path.getsize(os.path.join(DATA_FOLDER, 'interim', 'train.pkl')) / 1024 / 1024))\n",
    "print('Size of val.pkl file: {:.2f} MB'.format(os.path.getsize(os.path.join(DATA_FOLDER, 'interim', 'val.pkl')) / 1024 / 1024))\n",
    "print('Size of test.pkl file: {:.2f} MB'.format(os.path.getsize(os.path.join(DATA_FOLDER, 'interim', 'test.pkl')) / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 462221\n",
      "Val size: 57778\n",
      "Test size: 57778\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(os.path.join(DATA_FOLDER, 'interim', 'train.pkl'))\n",
    "val = pd.read_pickle(os.path.join(DATA_FOLDER, 'interim', 'val.pkl'))\n",
    "test = pd.read_pickle(os.path.join(DATA_FOLDER, 'interim', 'test.pkl'))\n",
    "\n",
    "assert len(train['reference']) == len(train['translation']) == train['ref_tox'].shape[0] == train['trn_tox'].shape[0]\n",
    "assert len(val['reference']) == len(val['translation']) == val['ref_tox'].shape[0] == val['trn_tox'].shape[0]\n",
    "assert len(test['reference']) == len(test['translation']) == test['ref_tox'].shape[0] == test['trn_tox'].shape[0]\n",
    "\n",
    "print('Train size: {}'.format(len(train['reference'])))\n",
    "print('Val size: {}'.format(len(val['reference'])))\n",
    "print('Test size: {}'.format(len(test['reference'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
